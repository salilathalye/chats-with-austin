{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CWA Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBzs63XEvYY74sm+l/7Nqn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salilathalye/chats-with-austin/blob/main/CWA_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqI8pM4VelCz"
      },
      "source": [
        "# Data Science Deliberate Practice - Binary Logistic Regression\n",
        "<p>Dependent Variable\n",
        "\n",
        "\n",
        "*   Categorical\n",
        "*   Binary-valued e.g. Case | No Case, 1 | 0, Success | Failure\n",
        "\n",
        "<p>Independent Variables\n",
        "\n",
        "\n",
        "*   Continuous \n",
        "*   Binary\n",
        "*   Categorical coded as dummy/indicator variables\n",
        "\n",
        "<p> Approach\n",
        "Treats the dependent variable as an outcome of a Bernoulli trial\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C01CRm20Ms5H"
      },
      "source": [
        "###Bernoulli Trial\n",
        "$$Y_i|x_{1,i},x_{1,i},x_{1,i},\\ldots,x_{m,i} \\sim Bernoulli(p_i)$$\n",
        "Outcomes $Y_i$ conditioned on the explanatory variables follow a Bernoulli distribution with parameter $p_i$\n",
        "\n",
        "$$E[Y_i|x_{1,i},x_{1,i},x_{1,i},\\ldots,x_{m,i}] = p_i$$\n",
        "The expected value of $Y_i$ converges to the the probability of success $p_i$\n",
        "\n",
        "$$Pr(Y_i = y_i|x_{1,i},x_{1,i},x_{1,i},\\ldots,x_{m,i}) = p_i^{y_i}(1-p_i)^{(1-yi_i)}$$\n",
        "The Probability Mass Function (PMF) of the Bernoulli Trial takes the values $p_i$ for Success $(y_i = 1)$ or $(1-p_i)$ for Failure $(y_i = 0)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZEpsCDSKe-R"
      },
      "source": [
        "####Binomial Distribution\n",
        "#####Binomial Likelihood Function\n",
        "<p>n tries, r successes given probability p of success\n",
        "<p>\n",
        "$$L(p) = {n \\choose r} p^r(1-p)^{(n-r)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk5bed2kEtSv"
      },
      "source": [
        "####Odds Ratio\n",
        "p is the probability of the positive event\n",
        "$$0 \\lt \\frac{p}{(1-p)} \\lt 1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAh82LhyFgQU"
      },
      "source": [
        "####Logit aka Log-Odds\n",
        "$$logit(p) = log_e \\frac{p}{(1-p)}$$\n",
        "\n",
        "$log_e$ also written as $ln$ is the natural logarithm, the inverse of $log_e(x)$ is $e^{x}$, where $e$ is Euler's Number.\n",
        "\n",
        "\n",
        "\n",
        "Why are we concerned with log-odds? Taking the natural log of the odds ratio provides a continous outcome. The logit serves as a link function converting the binary outcome into a continuous outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah8z2Oj2Gvnu"
      },
      "source": [
        "###Linear Functions of Explanatory Variables\n",
        "$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n$$\n",
        "\n",
        "$$y = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5kDdQlopQjK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cViSgR8uG41o"
      },
      "source": [
        "###Logistic Sigmoid Function aka Sigmoid Function\n",
        "\n",
        "The logistic function $F(t)$ takes on the values between 0 and 1\n",
        "\n",
        "$$F(t) = \\frac{e^t}{e^t + 1}$$\n",
        "\n",
        "Dividing the numerator and denominator by $e^t$ we get\n",
        "\n",
        "$$F(t) = \\frac{1}{1 + e^{-t}}$$\n",
        "\n",
        "\n",
        "$$\\phi(z) = \\frac{1}{1+e^{-z}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApwhHGC0ItpB"
      },
      "source": [
        "If we denote t as a linear function explanatory variables x\n",
        "$$F(t) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_nx_n)}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DLIDndYJnD0"
      },
      "source": [
        "####Model\n",
        "Logistic Regression models the log odds as a linear function of k factors (predictors). \n",
        "\n",
        "$${logit}(p(y=1|\\mathbf x)) = \n",
        "\\beta_0x_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_kx_k = \n",
        "\\sum\\limits_{i=0}^{k} {\\beta_ix_i} = \\boldsymbol \\beta^\\intercal \\mathbf x$$\n",
        "\n",
        "Here $p(y=1|\\mathbf x)$ represents the conditional probability that y belongs to class 1, given its k features represented in $\\mathbf x$.\n",
        "\n",
        "$\\beta_0$ is the intercept and $x_0$ is set to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YP27TaLJHV3"
      },
      "source": [
        "Here z is the net input i.e. the linear combination of the weights and input features $z = \\boldsymbol \\beta^\\intercal \\mathbf x$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkM_PL_oa7FA"
      },
      "source": [
        "$$\\hat y =\n",
        "\\left\\{\n",
        "    \\begin{array}{ll}\n",
        "        1  & \\mbox{if } \\phi(z) \\geq 0.5 \\\\\n",
        "        0 & \\mbox {otherwise}\n",
        "    \\end{array}\n",
        "\\right.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twUyuub4eLUE"
      },
      "source": [
        "###Learning the weights $\\beta_i$ - Maximum Likelihood Estimation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRMzHyvJjnVy"
      },
      "source": [
        "Likelihood of observing the data, given the model parameters, assuming the data \n",
        "follow a distribution\n",
        "\n",
        "$$P(X;\\theta)$$\n",
        "\n",
        "Find the set of parameters $\\theta$ that maximize the likelihood\n",
        "\n",
        "$$max L(X; \\theta)$$\n",
        "\n",
        "We have to do this across the set of n samples, so it is a joint probability of the conditional probability of each sample.\n",
        "\n",
        "$$max \\prod\\limits_{i=1}^{n} P(x_i;\\theta)$$\n",
        "When we multiple hundreds of small probabilities together we can encounter [arithmetic underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow). Taking the natural log converts the multiplication into a sum, it does not affect the computation of argmax (why?). This is called the log-likelihood function. Rather than maximize a function we prefer to minimize a function (first derivative = 0 at minimum) therefore we also negate the function. We have converted the problem from finding $\\theta$ that maximizes the joint probability to a problem of minimizing the negative log-likelihood.\n",
        "$$min  - \\sum\\limits_{i=1}^{n} log P(x_i;\\theta)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCTtU228B7pG"
      },
      "source": [
        "### Cost Function for Negative Log-Likelihood\n",
        "$$L(\\mathbf {w}) = P(\\mathbf y | \\mathbf x; \\mathbf w) = \\prod\\limits_{i=1}^{n}P(y^{(i)}|x^{(i)};\\mathbf w) = \\prod\\limits_{i=1}^{n}(\\phi(z^{(i)}))^{y^{(i)}}(1-\\phi(z^{(i)}))^{1 - y^{(i)}}$$\n",
        "\n",
        "Taking the natural logarithm, we convert the product into sum\n",
        "\n",
        "$$log L(\\mathbf w) = \\sum\\limits_{i}^{}y^{(i)}log(\\phi(z^{(i)})+(1-y^{(i)})log(1-\\phi(z^{(i)}))$$\n",
        "\n",
        "Reframing the problem to minimize the negative log-likelihood\n",
        "$$J(\\mathbf {w}) = - log L(\\mathbf w)$$\n",
        "\n",
        "$$J(\\mathbf {w}) = - \\sum\\limits_{i}^{}\\left[y^{(i)}log(\\phi(z^{(i)})+(1-y^{(i)})log(1-\\phi(z^{(i)}))\\right]$$\n",
        "\n",
        "$$J(\\mathbf {w}) = \\sum\\limits_{i}^{}\\left[-y^{(i)}log(\\phi(z^{(i)})-(1-y^{(i)})log(1-\\phi(z^{(i)}))\\right]$$\n",
        "\n",
        "$\\mathbf w\\$ are the weights we wish to learn, the superscript (i) represents each sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5C7sCpOOQJt"
      },
      "source": [
        "###Partial Derivative of the Sigmoid Function\n",
        "\n",
        "$$\\frac{\\partial {}}{\\partial z}\\phi(z) = \\frac{\\partial {}}{\\partial z}\\frac{1}{1+e^{-z}} = \\frac{1}{(1+e^{-z})^2}e^{-z}$$\n",
        "\n",
        "This is because\n",
        "\n",
        "$$\\frac{\\partial {}}{\\partial z}\\frac{1}{1+e^{-z}} = \\frac{\\partial {}}{\\partial z}({1+e^{-z}})^{-1} $$\n",
        "\n",
        "$$-1({1+e^{-z}})^{-2}\\frac{\\partial {}}{\\partial z}({1+e^{-z}})$$\n",
        "\n",
        "$$\\frac{1}{(1+e^{-z})^2}e^{-z}$$\n",
        "\n",
        "$$\\frac{1}{(1+e^{-z})^2}((1 + e^{-z}) -1) = \\frac{1}{1+e^{-z}} \\left(1 - \\frac{1}{1+e^{-z}}\\right) = \\phi(z)(1-\\phi(z))$$\n",
        "\n",
        "Therefore\n",
        "\n",
        "$$\\frac{\\partial {}}{\\partial z}\\phi(z) = \\phi(z)(1-\\phi(z))$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OolU6zpQbR_X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyirqmK_e0JU"
      },
      "source": [
        "###References\n",
        "*Raschka, Mirjalili Python Machine Learning Third Edition, Pakt\n",
        "*Burkov, Andriy The Hundred-Page Machine Learning Book\n",
        "*Abhishek Thakur, Approaching (Almost) Any Machine Learning Problem\n",
        "*Geron, Aurelien Hands-On Machine Learning with Scikit-Learn & Tensorflow\n",
        "*Strickland, Jeffrey Logistic Regression Inside-Out"
      ]
    }
  ]
}